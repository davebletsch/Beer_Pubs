{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from more_itertools import unique_everseen\n",
    "import string\n",
    "import itertools\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from db import db, query, query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping_utilities import (get_missing_scrape_targets,\n",
    "                                create_scraper, \n",
    "                                iterate_scraping,\n",
    "                                make_db_inserter,\n",
    "                                get_soup_with_requests,\n",
    "                                get_selenium_resource\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_table_name   = 'beers'\n",
    "bars_url_col_name = 'bar_url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The relevant table will be search_pages and the column \n",
    "#  search_page_url will include urls already scraped\n",
    "search_page_url_col_name = 'search_page_url'\n",
    "search_page_table_name   = 'search_pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beer_scraping import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Get a list of bars on beermenus.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this by using their \"places\" page which has a list of bars, 20 per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, uncomment\n",
    "# query(f'DROP TABLE {search_page_table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of urls to search\n",
    "@np.vectorize\n",
    "def get_search_page_url(page_num):\n",
    "    return f'https://www.beermenus.com/places?page={page_num}'\n",
    "\n",
    "target_urls  = get_search_page_url(range(1,32))\n",
    "missing_urls = get_missing_scrape_targets(target_urls,\n",
    "                                          search_page_url_col_name,\n",
    "                                          search_page_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_search_page(soup, **kwargs):\n",
    "    '''\n",
    "        Parse the beautiful soup on a beermenus.com search page\n",
    "        \n",
    "        Given the soup, select the search results and get the links\n",
    "        Return a data frame with columns:\n",
    "         - search_page_url, the source\n",
    "         - link, the bar page scraped\n",
    "         - bar_name, the website text displayed\n",
    "         \n",
    "        Parameters:\n",
    "         - soup: a beautiful soup object from the page\n",
    "         - **kwargs: url is required, should be the\n",
    "            source url of the page. All else ignored.\n",
    "        \n",
    "        Returns:\n",
    "         - DataFrame with the parsed data\n",
    "                                         \n",
    "    '''\n",
    "    search_page_url = kwargs['url']\n",
    "    \n",
    "    # Use a CSS selector to find the list of bars\n",
    "    links = soup.select('body div.results ul.pure-list a')\n",
    "    data  = [{search_page_url_col_name  : search_page_url,\n",
    "              'link'                    : 'https://www.beermenus.com' + link.attrs['href'],\n",
    "              'bar_name'                : link.getText()}\n",
    "             for link in links]\n",
    "    return pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_inserter = make_db_inserter(search_page_table_name)\n",
    "scraper = create_scraper(parse_search_page, db_inserter, get_soup_with_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_return = iterate_scraping(scraper, missing_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scrape the bar pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed the development and analysis process, there is some information about the bar that isn't collected here. A seperate loop is used to collect that information into a 'bars' table in Part 3. Some bar information is collected, again trading good database practices (being in database normal form) for agility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser is relatively complex in order to handle the variety of cases and can be found in beer_scraping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_inserter = make_db_inserter('beers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_urls = query_list('link', search_page_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_urls = get_missing_scrape_targets(target_urls,\n",
    "                                          bars_url_col_name,\n",
    "                                          bars_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beer stores with ~1000 beers or more. They will be excluded from the analysis\n",
    "#  and take a long time to parse\n",
    "exclusions = [\n",
    "'https://www.beermenus.com/places/39689-public-wine-beer-and-spirits',\n",
    "'https://www.beermenus.com/places/2190-bellmore-beverage',\n",
    "'https://www.beermenus.com/places/20239-fast-break',\n",
    "'https://www.beermenus.com/places/11448-castle-wine-spirits',\n",
    "'https://www.beermenus.com/places/5797-universal-beverage-llc',\n",
    "'https://www.beermenus.com/places/14346-super-buy-rite-of-north-plainfield',\n",
    "'https://www.beermenus.com/places/20828-beer-town',\n",
    "'https://www.beermenus.com/places/51031-boardwalk-liquids',\n",
    "'https://www.beermenus.com/places/31794-the-wine-guys',\n",
    "'https://www.beermenus.com/places/43312-linwood-wine-liquor-at-hudson-lights',\n",
    "'https://www.beermenus.com/places/47185-bloomfield-buyrite-we-deliver',\n",
    "'https://www.beermenus.com/places/14981-beverage-plus-2',\n",
    "'jjj',\n",
    "'https://www.beermenus.com/places/17349-other-half-brewing-company',\n",
    "'https://www.beermenus.com/places/1868-on-tap-at-whole-foods-market-columbus-circle',\n",
    "'https://www.beermenus.com/places/23644-midland-brew-house', \n",
    "'https://www.beermenus.com/places/25864-bridge-view-tavern-beer-garden',\n",
    "'https://www.beermenus.com/places/41075-icarus-brewing',\n",
    "'https://www.beermenus.com/places/50163-huertas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_urls = set(missing_urls).difference(exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_getter = get_selenium_resource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = create_scraper(parse_page, beer_inserter, page_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_data = iterate_scraping(scraper, missing_urls, on_fail = 'proceed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Get additional bar information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_urls = query_list('bar_url', bars_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_inserter = make_db_inserter('bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 pages already scraped detected\n",
      "0 pages needing scraping detected\n"
     ]
    }
   ],
   "source": [
    "missing_urls = get_missing_scrape_targets(target_urls,\n",
    "                                          'bar_url',\n",
    "                                          'bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = create_scraper(parse_bar_info, bar_inserter, get_soup_with_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_data = iterate_scraping(scraper, missing_urls, on_fail = 'abort')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
